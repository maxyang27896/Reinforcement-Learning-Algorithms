{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Q-Learning with OpenAI GymTaxi Environment\n",
    "The following code is solving the reinforcement learning problem of self-driving cab in a simplified environment of the OpenAI gym environment [Taxi-v3](https://gym.openai.com/envs/Taxi-v2/). For this, the Q-Learning algorithms was used to create a Q-table of size (state_space_size, action_space_size) which presents the action selection process. This is a tutorial to demonstrate a Q-learning algorithm to deal with discrete state space and discrete action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import random \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "The objective of this environment is for the taxi\n",
    "* drive to the customer \n",
    "* pick up the customer, \n",
    "* drive to the destination \n",
    "* drop off the customer\n",
    "\n",
    "The environment has 500 discrete possible states defining the location of the taxi, the customer and destination, and 6 actions spaces (up, down, left, right, pickup, drop off). The objective of the Q-table is to learn the Q-values for each action at each state and allow the algorithm to pick the best action in order to obtain the highest reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the taxi environment \n",
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Discrete(500)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "The next state:  169\n",
      "Reward:  -1\n",
      "Terminal state False\n",
      "Probability of happening:  {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Display the number of action and states in the environment \n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "# Run a random action at a state\n",
    "env.reset()\n",
    "env.render()\n",
    "state, reward, done, info = env.step(1)\n",
    "env.render()\n",
    "\n",
    "# print information\n",
    "print(\"The next state: \", state)\n",
    "print(\"Reward: \", reward)\n",
    "print(\"Terminal state\", done)\n",
    "print(\"Probability of happening: \",info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "* The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "* R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "# Return the state number for a particular environemnt setting\n",
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "# set the environment to this state\n",
    "env.s = state\n",
    "\n",
    "# render to show the environment in this state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is obtained information at a particular state. This means that at a state, there are 6 actions to take (0, 1, 2, 3, 4, 5) and for each action, it tells us that \n",
    "* There is only 1 outcome with a probability of 1. \n",
    "* The next state number of this action. \n",
    "* The reward received with this action\n",
    "* And whether this next state is a terminal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state. The values store in the Q-table are called a Q-values, and they map to a (state, action) combination. The Q-table is initialised as zeros(state_size, action_size) and is updated using the Q-learning algorithm after each state action pair and with the reward received. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/q_matrix.png\" style=\"width:400px;height400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The q-value iteration is derived from bellman's equation to iteratively update the q-value of a state and action pair based on a step of state, action, reward. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Q(state, actions) \\leftarrow (1-\\alpha)Q(state, actions) + \\alpha(reward + \\gamma \\max_{a}(Q(next\\_state, all\\_actions))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty table of zeros \n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for N number of episodes of training\n",
    "N = 10000\n",
    "for i in range(0, N):\n",
    "    \n",
    "    # reset environment \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # loop until the agent exits the environment (terminal point/drop off passenger)\n",
    "    while not done:\n",
    "        \n",
    "        # epsilon greedy explore and exploit\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        # take the action and get q-value of next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        max_q_next = np.max(q_table[next_state])\n",
    "        \n",
    "        # update the q-value for current state\n",
    "        q_table[state, action] = (1-alpha)*q_table[state,action] + alpha*(reward + gamma*max_q_next)\n",
    "        \n",
    "        # set the current state as next state\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "The final reward:  13\n",
      "Number of steps taken:  8\n"
     ]
    }
   ],
   "source": [
    "# simulate a result\n",
    "state = env.reset()\n",
    "done = False \n",
    "reward_sum = 0\n",
    "step = 0 \n",
    "# until done \n",
    "while not done:\n",
    "    \n",
    "    # render the current environment \n",
    "    env.render()\n",
    "    \n",
    "    # take an action in the max q_table\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # set the next state\n",
    "    state = next_state\n",
    "    \n",
    "    # Cumulate rewards and record step\n",
    "    reward_sum += reward\n",
    "    step+= 1\n",
    "    \n",
    "    # pause for next action\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"The final reward: \", reward_sum)\n",
    "print(\"Number of steps taken: \", step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
