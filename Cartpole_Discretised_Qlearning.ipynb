{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Q-learning Cartpole Discretised\n",
    "The following code uses the q-table learning on a continous state space of the cartpole environment by using an observation wrapper. This discretises the 4-dimensional continuous state-space into dicretised space without losing any information. The objective is to balance the cartpole for as long as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the taxi environment \n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation is is a 4-dimentional continuous space of (cart position, cart velocity, pole angle, pole velocity at tip) and the actions are (0,1) to move the cart left and right. To allow the use of a q-table, it is required to conver this continous observation space into discretised space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"This wrapper converts a Box observation into a single integer.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins=10, low=None, high=None):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "\n",
    "        low = self.observation_space.low if low is None else low\n",
    "        high = self.observation_space.high if high is None else high\n",
    "\n",
    "        self.n_bins = n_bins\n",
    "        self.val_bins = [np.linspace(l, h, n_bins + 1) for l, h in\n",
    "                         zip(low.flatten(), high.flatten())]\n",
    "        self.observation_space = gym.spaces.Discrete((n_bins + 2) ** (low.flatten().shape[0]))\n",
    "\n",
    "    def _convert_to_one_number(self, digits):\n",
    "        return sum([d * ((self.n_bins + 1) ** i) for i, d in enumerate(digits)])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        digits = [np.digitize([x], bins)[0]\n",
    "                  for x, bins in zip(observation.flatten(), self.val_bins)]\n",
    "        return self._convert_to_one_number(digits)\n",
    "\n",
    "# create the new environment\n",
    "env = DiscretizedObservationWrapper(\n",
    "    env, \n",
    "    n_bins=8, \n",
    "    low=np.array([-2.4, -2.0, -0.42, -3.5]), \n",
    "    high=np.array([2.4, 2.0, 0.42, 3.5])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Discrete(10000)\n"
     ]
    }
   ],
   "source": [
    "# check the new action space and observation space\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The q-table is of size (observation_space_size, action_space_size):  (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# create an empty table of zeros \n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print('The q-table is of size (observation_space_size, action_space_size): ',(len(q_table), len(q_table[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.5\n",
    "gamma = 0.99\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise reward accumulation\n",
    "avg_rewards = 0\n",
    "N = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1000 average score of past 1000 episodes: 112.78200000000001\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "# Loop for 15000 of episodes to train\n",
    "for i in range(1,1001):\n",
    "    \n",
    "    # reset environment \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    \n",
    "    # loop until the agent exits the environment (terminal point/drop off passenger)\n",
    "    while not done:\n",
    "        \n",
    "        # epsilon greedy explore and exploit\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        # take the action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        max_q_next = np.max(q_table[next_state])\n",
    "        \n",
    "        # update the q-value for state\n",
    "        q_table[state, action] = (1-alpha)*q_table[state,action] + alpha*(reward + gamma*max_q_next)\n",
    "        \n",
    "        # set the current state as next state\n",
    "        state = next_state\n",
    "        \n",
    "        # accumulate rewards per episode\n",
    "        total_rewards += reward\n",
    "        \n",
    "    # averaging total rewards\n",
    "    N += 1\n",
    "    avg_rewards = avg_rewards + 1.0/(N) * (total_rewards - avg_rewards)\n",
    "        \n",
    "    # print out some average rewards every 1000 epsiodes\n",
    "    if (i % 1000) == 0:\n",
    "        print(\"Run: \" + str(i) + \" average score of past 1000 episodes: \" + str(avg_rewards))\n",
    "        N = 0\n",
    "        avg_rewards = 0\n",
    "\n",
    "print('Done Training!')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5000 the average score is 182.21959999999933\n"
     ]
    }
   ],
   "source": [
    "# function to test the result of the q_table for n_episodes\n",
    "def test(q_table, n_episodes):\n",
    "    \n",
    "    # store average rewards\n",
    "    avg_rewards = 0\n",
    "    \n",
    "    for i in range(1, n_episodes+1):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False \n",
    "        total_rewards = 0\n",
    "        \n",
    "        # until done \n",
    "        while not done:\n",
    "            \n",
    "            # take an action in the max q_table\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # acculmulate rewards\n",
    "            total_rewards += reward\n",
    "        \n",
    "        avg_rewards = avg_rewards + 1/(i) * (total_rewards - avg_rewards)\n",
    "\n",
    "    print(\"After \" + str(n_episodes) + \" the average score is \" + str(avg_rewards))\n",
    "            \n",
    "# run test for 5000 episodes\n",
    "test(q_table, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
