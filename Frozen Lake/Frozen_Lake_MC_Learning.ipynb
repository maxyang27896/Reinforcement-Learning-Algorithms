{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Reinforcement Learning on Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code solves the frozen lake OpenAI Gym environment by using Monte Carlo reinforcement learning. This learns directly from episodes of experience without any prior knowledge of MDP transitions. This code implements the monte carlo first visit with epsilon soft method, which averages the only first visit of the epsisode and uses an epsilon greedy approach to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment \n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size:  16\n",
      "Action size:  4\n"
     ]
    }
   ],
   "source": [
    "# Get state space and action space size\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print('State size: ', state_size)\n",
    "print('Action size: ', action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S = start  \n",
    "F = frozen, can be walked on  \n",
    "H = hole, will end the episode  \n",
    "G = goal, finishing point with reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Action: 1\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "Action: 2\n",
      "[(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "Action: 3\n",
      "[(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "# Information on the actions at state s\n",
    "info = env.P[s]\n",
    "for key, value in info.items():\n",
    "    print('Action:',  key,)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment transition probabilities show that the agent is equally likely to end up in 3 different states following a particular action, meaning the agent is unlikely to act as to desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Algorithm\n",
    "This algorithm involves two steps:  \n",
    "* Policy evaluation: given a fixed policy (state -> action), return the expected reward for each (state, action) pair following this policy in one episode\n",
    "* Policy improvement: given the accumulated rewards of each (state, action) pair, update the Q-table by using a running average of Q-value of each (state, action) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.9      # discount factor\n",
    "epsilon = 0.1    # exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "Evaluates a fixed policy with a degree of exploration.   \n",
    "sar_list = [(state, action, reward), ....]  \n",
    "sag_list = [(state, action, expected reward), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy):\n",
    "    '''\n",
    "    Function to evaluate a policy and outputs\n",
    "    a dictionary that provides the expected \n",
    "    discounted rewards from that state, taking \n",
    "    that action\n",
    "    \n",
    "    input: policy[state] -> action\n",
    "    output: sag_list[state, action] -> expected discounted rewards \n",
    "    '''\n",
    "    \n",
    "    # Initialise the return list\n",
    "    G = 0\n",
    "    sag_list = []\n",
    "    \n",
    "    # loop until an episode with valid reward is found i.e until the goal has been reached by acting randomly\n",
    "    while not sag_list:\n",
    "    \n",
    "        # Initialise episode\n",
    "        state = env.reset()\n",
    "        action = policy[state]\n",
    "        sar_list = [(state, action, 0)]\n",
    "        \n",
    "        done = False \n",
    "        reward_sum = 0\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            # take an action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Append state, action, reward to visitied list\n",
    "            if done:\n",
    "                \n",
    "                # Increase the final reward if goal is reached and negative reward if fall into a hole  \n",
    "                reward = reward*10 if reward !=0 else -1\n",
    "                \n",
    "                # append state, action, reward\n",
    "                sar_list.append((next_state, None, reward))\n",
    "            else:\n",
    "                \n",
    "                # epsilon greedy action\n",
    "                action = policy[next_state] if np.random.uniform(0,1) > epsilon else env.action_space.sample()\n",
    "                \n",
    "                # append state, action, reward\n",
    "                sar_list.append((next_state, action, reward))\n",
    "               \n",
    "            # Cumulating values\n",
    "            reward_sum += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Backpropagate rewards only if last reward exists\n",
    "        if sar_list[-1][-1]:\n",
    "            for state, action, reward in sar_list[::-1]:\n",
    "                G = reward + gamma * G\n",
    "                sag_list.append((state, action, G))\n",
    "                sag_list.reverse()\n",
    "                \n",
    "    return sag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "Following a policy evaluation and a cumulated reward table was obtained for (state, action) pair, the Q-table can be updated with new averages of Q-value. First visit monte carlo averaging was applied here. At the end, the policy is updated using the new values in the Q-table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training \n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "N_visit = np.zeros((state_size, action_size))\n",
    "policy = np.random.randint(0,action_size, size=(state_size))\n",
    "N = 5000\n",
    "\n",
    "# loop for N episodes\n",
    "for _ in range(0, N):\n",
    "    \n",
    "    # Initialise for each episode\n",
    "    sag_list = policy_evaluation(policy)\n",
    "    visited = set()\n",
    "    \n",
    "    # Policy improvement loop\n",
    "    for state, action, G in sag_list:\n",
    "        \n",
    "        # Add to the visited list (MC first visit)\n",
    "        if (state, action) not in visited:\n",
    "            N_visit[state][action] += 1\n",
    "            visited.add((state, action))\n",
    "            \n",
    "            # Update q-table value with the largest G for that (state, action) pair\n",
    "            Q_table[state][action] = Q_table[state][action] + 1/N_visit[state][action]*(G - Q_table[state][action])\n",
    "    \n",
    "    # Update policy table\n",
    "    for s in range(0, state_size):\n",
    "        policy[s] = np.argmax(Q_table[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(policy):\n",
    "    '''\n",
    "    Function to run one game\n",
    "    input: policy table\n",
    "    return: sum of reward \n",
    "    '''\n",
    "    \n",
    "    # initialise\n",
    "    state = env.reset()\n",
    "    done = False \n",
    "    reward_sum = 0\n",
    "    \n",
    "    # until done \n",
    "    while not done:\n",
    "        \n",
    "        # take an action in the max q_table\n",
    "        action = policy[state]\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # accumulate rewards\n",
    "        reward_sum += reward\n",
    "        \n",
    "    return reward_sum  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, r):\n",
    "    '''\n",
    "    Function to test a policy r times\n",
    "    input: a policy table, and r repeated times\n",
    "    return: percentage of wins \n",
    "    '''\n",
    "    \n",
    "    # Initialise win\n",
    "    wins = 0\n",
    "    \n",
    "    # loop for r times\n",
    "    for i in range(r):\n",
    "        \n",
    "        # run games \n",
    "        reward = run_game(policy)\n",
    "        \n",
    "        # accumulate winning games\n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "                \n",
    "    return wins / r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1000 games played, the winning percentage is 43.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"Out of {} games played, the winning percentage is {}%\".format(1000, test_policy(policy, 1000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
